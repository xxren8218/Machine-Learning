HDFS **分布式存储**   与spark storm计算框架组合  HBase数据库  ——管理数据的

分布式结构 master slave

name node  client 负责文件的拆分  128MB  3份（默认）

data node

MapReduce **分布式计算**  离线计算 2.X之前 速度比较慢 对比spark （离线计算——在1s之内没有响应）

编程思想 Map  分（计算是在各个节点上进行）  Reduce 合

hadoop streaming  —— 可以通过python脚本写MR，MRJob——对Hadoop streaming封装。

Yarn 是资源管理 工具  管cpu 内存  MapReduce spark 分布式计算 ——管理内存的。

- 将 MapReduce or spark 任务分发到哪个节点。
- 需要计算时才与其有关，只存储不需要。

RM NM AM

社区版 CDH



### 什么是Hive

- 基于Hadoop 数据保存到HDFS（自己不管数据存储，放在HDFS上。与MySQL不一样）

- 数据仓库工具

- 结构化的数据 (**规则的数据**) 映射为一张数据库表

  01,张三,89

  02,李四,91

  03,赵武,92

- HQL查询功能 (Hive SQL)

- 本质 把HQL翻译成MapReduce 降低使用hadoop计算的门槛——相当于给MR套了壳子。

- 离线数据分析开发效率比直接用MapReduce 高

  

### Hive架构

- 用户接口：shell命令行
- 元数据存储
  - 数据库 表 都保存到哪些位置上
  - 表中的字段名字 类型
  - mysql derby(自带)
- Drive
  - 负责把HQL翻译成mapreduce
  - 或者翻译成 shell 命令

### Hive和Hadoop关系

- 利用hdfs存数据 利用mr算
- Hive只需要跟 Master节点打交道 不需要集群

### Hive和关系型数据库区别

- hive （离线计算） 海量查询
- hive最主要做查询 不涉及删除修改 默认不支持删除修改，默认不支持事务，并不完全支持标准sql
- sql CRUD全部支持， 支撑（在线业务），索引完整 支持事务



### Hive 基本使用

- 创建表

```sql
CREATE TABLE student(classNo string, stuNo string, score int) row format delimited fields terminated by ',';
```

- 字段不需要指定占多少字节

- 需要通过row format delimited fields terminated by ','指定列的分隔符 

- 加载表数据的时候尽量使用 load data方式 把整个文件put上去

  - 不要用insert:因为一句insert一个MR，很慢。

  ```sql
  load data local inpath '/home/hadoop/tmp/student.txt'overwrite into table student;
  ```

- 内部表和外部表

  - managed table

    - 建表语句

      ```sql
      CREATE TABLE 表名（字段名 字段类型，）row format delimited fields terminated by ','
      ```

    - 删除表

      元数据和数据一起删除

    - 数据位置

      - 默认是/user/hive/warehouse

  - external table

    - 建表语句

      ```sql
      CREATE External TABLE 表名（字段名 字段类型，）row format delimited fields terminated by ',' location '数据在hdfs上的路径';
      ```

    - 删除表

      - 只删除元数据 数据会保留

    - 数据位置
    
      - 数据可以在hdfs上的任意位置

- 分区表

  - 当数据量比较大的时候，使用分区表可以缩小查询的数据范围

  - 分区表实际上就是在表的目录下创建的子目录

  - 如果有分区表的话查询的时候，尽量要使用分区字段

  - 创建分区表的语句

    ```sql
    create table 表名 (字段名，字段类型....) partitioned by (分区字段名 分区字段类型) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
    ```

  - 向分区表中插入数据

    ```sql
    load data local inpath '/home/hadoop/tmp/employee.txt' into table 表名 partition(分区字段名字='分区的具体值');
    ```

  - 添加分区

    ```sql
    alter table 表名 add if not exists partition(分区字段名字='分区的具体值');
    ```

  - 动态分区

    - 插入数据的时候指定分区的字段，会自动帮助创建分区所对应的文件夹

    - 需要关掉默认设置

      ```sql
      set hive.exec.dynamic.partition.mode=nonstrict;
      ```

### UDF自定义函数

- hive提供的函数不能满足需求的时候就可以使用自定函数

  - 使用别人已经编译好的.jar

    - jar加到 hive环境中

    - jar 可以在hdfs上 也可是在centos 上

    - 创建一个临时函数

      ```sql
      CREATE TEMPORARY FUNCTION 自定义函数名字 as '自定义函数在jar包中的包名'
      ```

    - 创建一个永久函数

      ```sql
      CREATE FUNCTION 自定义函数名字 as '自定义函数在jar包中的包名' using jar 'jar位置';
      ```

      

  - 自己写python脚本实现udf、udaf

    - add file python文件的位置

    - ```sql
      SELECT TRANSFORM(fname, lname) USING 'python udf1.py' AS (fname, l_name) FROM u;
      ```

      

### 综合案例

- collect_set/collect_list

  - group by之后 针对某一列聚合 结果放到[]
  - 区别 一个去重 一个不去重

- lateral view explode

  - explode函数 把复杂数据类型 array map 拆开 一行变多行

  - lateral view 和explode函数 配合使用 创建虚拟视图 可以把explode的结果和其它列一起查询

  - ```sql
    select article_id,kw from articles
    lateral view outer explode(key_words) t as kw
    ```

- CONCAT, CONCAT_WS

  - 不同列的字符串拼接到一起
  - concat_ws 可以把array中的元素拼接到同一个字符串中 指定分割符

- str_to_map 把具有key:value形式的字符串转换成map

### sqoop 介绍

- 作用 数据交换工具 可以实现 数据在mysql oracle<==> hdfs之间互相传递

- 原理 通过写sqoop 命令 把sqoop命令翻译成mapreduce 通过mapreduce连接各种数据源 实现数据的传递

- 通过sqoop 把数据从mysql导入到hdfs

  - sqoop import --connect jdbc:mysql://mysql数据库地址:3306/数据库名字 --username root --password password --table 要导出数据的表名 -m mrjob的数量
  -  默认会把文件导入到 hdfs上 /user/linux用户名 文件夹下
  - 通过 --target-dir指定其它位置

  

  

### HBase介绍

- 分布式开源数据库
- 面向列
- Big Table开源实现
- 适合非结构化数据的存储

- PB级别数据
- 可以支撑在线业务
- 分布式系统特点 ：易于扩展，支持动态伸缩，并发数据处理

### 面向列数据库

- 关系型数据库：行式存储 每一行数据都是连续的 所有的记录都放到一个连续的存储空间中
- 列数据库： 列式存储 每一列对应一个文件 不同列并不对应连续的存储空间

- 结构化数据 V.S. 非结构化数据
  - 结构化数据
    - 预定义的数据模型  模型一旦确定不会经常变化（表结构不会频繁调整）
  - 非结构化数据
    - 没有预定义数据模型
    - 模型不规则 不完整
    - 文本 图片 视频 音频
- Hive 和 Hbase区别
  - hive hbase 共同点
    - 都可以处理海量数据
    - 文件都是保存到hdfs上
  - hive 和 hbase不同
    - 计算不是通过mapreduce实现的 自己实现的CRUD功能
    - hive 通过mapreduce实现 数据查询的
    - hbase 可以有集群 集群的管理是通过zookeeper实现
    - hive 只能做离线计算
    - hbase 提供对数据的随机实时读/写访问功能
- HBase 对事务的支持 只支持行级别的事务
- CAP定理 
  - 分区容错性 分布式系统都要有的特性，任何时候都要能提供服务 P保证
  - HBase CP系统 强一致性

### Hbase 数据模型

- NameSpace 对应 关系型数据库 database  
- 表(table)：用于存储管理数据，具有稀疏的、面向列的特点。
- 行 （row）： 每一行都对应一个row key 行键 Hbase有索引但是只是在行键 rowkey有索引
- 列  Column family 和 Column qualifier 组成
- 列族（ColumnFamily）保存的就是 键值对集合 key:value
- 时间戳(TimeStamp)：是列的一个属性



### Hbase 和 传统关系型数据库区别

- 创建HBase表的时候只需要指定表名 和 列族

- 每一个行当中 只需要列族相同就可以了 至于每个列族中的 key:value对 key可以完全不同

- 

- 

  ## HBase基础架构

- Client
- Zookeeper
  - 保证HMaster有一个活着
  - HRegionServer HMaster地址存储
  - 监控Region Server状态 将Region Server信息通知HMaster
  - 元数据存储
- HMaster
- HRegionServer
- HStore
  
  - 每一个column family 对应了一个HStore
- HRegion
-  HLog



面向列数据库 列式存储

适合存非关系型数据

hbase 创建表的过程很简单 只需要指定表名和列族的名字就可以了

create '表名','列族名字'

NameSpace -》数据库

table

row-key 行键  hbase的索引只在 row-key才有

column family 列族 key：value  这里面key 又叫 column quanlifier

不同行的 相同的column family 中  column quanlifier可以完全不同



组件

- HMaster

- HRegionServer
  - HRegion
    - Hstore (一个列族对应)
      - memstore
      - storefile

如果遇到 hdfs safe mode

通过 hdfs dfsadmin -safemode leave

hive 一定要先启动元数据服务

- hive --service metastore&